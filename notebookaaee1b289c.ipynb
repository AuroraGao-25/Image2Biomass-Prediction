{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-20T05:56:27.441776Z",
     "iopub.status.busy": "2025-11-20T05:56:27.441454Z",
     "iopub.status.idle": "2025-11-20T05:56:28.069218Z",
     "shell.execute_reply": "2025-11-20T05:56:28.068343Z",
     "shell.execute_reply.started": "2025-11-20T05:56:27.441752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------- 1. 配置全局参数 --------------------------\n",
    "# 设备配置（自动识别GPU/CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "\"\"\"\n",
    "# 数据路径（Kaggle竞赛数据默认路径，无需修改）\n",
    "TRAIN_IMAGE_DIR = \"/kaggle/input/csiro-biomass/train\"\n",
    "TRAIN_CSV_PATH = \"/kaggle/input/csiro-biomass/train.csv\"\n",
    "TEST_CSV_PATH = \"/kaggle/input/csiro-biomass/test.csv\"\n",
    "SUBMISSION_CSV_PATH = \"/kaggle/input/csiro-biomass/sample_submission.csv\"\n",
    "\"\"\"\n",
    "TRAIN_IMAGE_DIR = \"./train\"\n",
    "TEST_IMAGE_DIR = \"./test\"\n",
    "TRAIN_CSV_PATH = \"./train.csv\"\n",
    "TEST_CSV_PATH = \"./test.csv\"\n",
    "SUBMISSION_CSV_PATH = \"./sample_submission.csv\"\n",
    "# 训练参数\n",
    "BATCH_SIZE = 32  # 根据GPU显存调整（Kaggle免费GPU建议32/64）\n",
    "EPOCHS = 20  # 可根据验证集性能调整\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5  # 防止过拟合\n",
    "NUM_TARGETS = 5  # 竞赛需预测的5类生物量\n",
    "\n",
    "# 定义目标变量名称列表（需与竞赛要求一致）\n",
    "target_names = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------- 2. 数据预处理与Dataset定义 --------------------------\n",
    "# 图像预处理（训练集添加数据增强，测试集仅基础预处理）\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet默认输入尺寸\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转\n",
    "    transforms.RandomVerticalFlip(p=0.3),  # 随机垂直翻转\n",
    "    transforms.RandomRotation(degrees=15),  # 随机旋转±15°\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 亮度/对比度调整\n",
    "    transforms.ToTensor(),  # 转为Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet预训练均值\n",
    "                         std=[0.229, 0.224, 0.225])    # ImageNet预训练方差\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 自定义Dataset类（适配竞赛数据格式）\n",
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None, is_test=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df = self.df.fillna(0.0)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # 统一处理sample_id：训练集和测试集的sample_id带有后缀\n",
    "        if is_test:\n",
    "            self.sample_ids = self.df['sample_id'].tolist()\n",
    "        else: \n",
    "        # 训练集：将5类目标变量整理为矩阵（sample_id为key，5个target为value）\n",
    "        # 测试集：无需目标值，仅返回sample_id\n",
    "        # pivot_table： 透视表函数，按指定列分组，将某列的唯一值作为新列，对应的值填充到新列中\n",
    "            self.df['sample_id'] = self.df['sample_id'].str.split('_').str[0]\n",
    "            self.target_map = self.df.pivot_table(\n",
    "                index=\"sample_id\", # 按sample_id分组，每个sample_id对应一行\n",
    "                columns=\"target_name\", # 将target_name的唯一值（5个生物量名称）作为新列\n",
    "                values=\"target\" # 透视值：用target列的数值填充到新列对应的单元格\n",
    "            ).reset_index() # 把sample_id从索引列转换为普通列，确保后续能通过iloc[idx]按索引获取到sample_id\n",
    "            self.target_cols = self.target_map.columns[1:]  # 5类生物量列名\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 返回数据集的总样本数\n",
    "        return len(self.target_map) if not self.is_test else len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 按索引idx读取单个样本\n",
    "        if not self.is_test:\n",
    "            # 训练集：读取图像+对应的5个目标值\n",
    "            sample_id = self.target_map.iloc[idx][\"sample_id\"]\n",
    "            image_path = os.path.join(self.image_dir, f\"{sample_id}.jpg\")\n",
    "\n",
    "            target_values = self.target_map.iloc[idx][self.target_cols].values.astype(np.float32)\n",
    "            targets = torch.tensor(target_values, dtype=torch.float32)\n",
    "        \n",
    "            # 读取图像（确保为RGB格式）\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return (image, targets, sample_id)\n",
    "\n",
    "        else:\n",
    "            # 测试集：仅读取图像和sample_id（用于后续提交）\n",
    "            sample_id = self.sample_ids[idx]\n",
    "            sample_id = sample_id.split('_')[0]\n",
    "            image_path = os.path.join(self.image_dir, f\"{sample_id}.jpg\")\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return (image, sample_id)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 357\n",
      "训练集样本数: 285\n",
      "验证集样本数: 72\n",
      "测试集样本数: 5\n"
     ]
    }
   ],
   "source": [
    "# 构建训练集/验证集（按8:2分割）\n",
    "train_full_dataset = BiomassDataset(TRAIN_CSV_PATH, TRAIN_IMAGE_DIR, train_transform, is_test=False)\n",
    "print(f\"训练集样本数: {len(train_full_dataset)}\")\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_full_dataset, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "print(f\"训练集样本数: {len(train_dataset)}\")\n",
    "print(f\"验证集样本数: {len(val_dataset)}\")\n",
    "\n",
    "# 构建测试集\n",
    "test_dataset = BiomassDataset(TEST_CSV_PATH, TEST_IMAGE_DIR, test_transform, is_test=True)\n",
    "print(f\"测试集样本数: {len(test_dataset)}\")\n",
    "# DataLoader（批量加载数据）\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\py312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\miniforge3\\envs\\py312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\huimi/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:17<00:00, 5.97MB/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- 3. 模型定义（ResNet50适配多目标回归） --------------------------\n",
    "class ResNet50Biomass(nn.Module):\n",
    "    def __init__(self, num_targets=NUM_TARGETS):\n",
    "        super().__init__()\n",
    "        # 加载预训练ResNet50（冻结初始层，仅微调顶层）\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 冻结主干网络（可选：小数据集场景下避免过拟合，后续可解冻微调）\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 替换分类头为多目标回归头（ResNet50最后一层是1000维分类层）\n",
    "        in_features = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # 防止过拟合\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_targets)  # 输出5个目标值\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "# 初始化模型并移至设备\n",
    "model = ResNet50Biomass(num_targets=NUM_TARGETS).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\py312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- 4. 损失函数与优化器定义 --------------------------\n",
    "criterion = nn.MSELoss()  # 回归任务常用损失函数\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "# 学习率调度器（可选：根据验证集性能调整学习率）\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.34s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：353.4712\n",
      "保存最优模型，当前验证集损失：353.4712\n",
      "训练集损失：449.9516 | 验证集损失：353.4712\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.44s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：450.1906 | 验证集损失：353.7505\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：352.6447\n",
      "保存最优模型，当前验证集损失：352.6447\n",
      "训练集损失：437.1217 | 验证集损失：352.6447\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:13<00:00,  1.48s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：350.3970\n",
      "保存最优模型，当前验证集损失：350.3970\n",
      "训练集损失：421.8287 | 验证集损失：350.3970\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:13<00:00,  1.47s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：349.6241\n",
      "保存最优模型，当前验证集损失：349.6241\n",
      "训练集损失：414.0498 | 验证集损失：349.6241\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.41s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：428.9830 | 验证集损失：351.5719\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：346.6562\n",
      "保存最优模型，当前验证集损失：346.6562\n",
      "训练集损失：424.4240 | 验证集损失：346.6562\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.30s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：404.6816 | 验证集损失：347.4932\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.28s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：403.3181 | 验证集损失：347.1377\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.34s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：383.8875 | 验证集损失：347.0687\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.25s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：395.5000 | 验证集损失：349.4248\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.30s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：343.4409\n",
      "保存最优模型，当前验证集损失：343.4409\n",
      "训练集损失：392.9222 | 验证集损失：343.4409\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.29s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：342.4153\n",
      "保存最优模型，当前验证集损失：342.4153\n",
      "训练集损失：385.6656 | 验证集损失：342.4153\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.30s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：385.9091 | 验证集损失：342.5421\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.29s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：394.1371 | 验证集损失：343.3767\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:11<00:00,  1.32s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：371.6560 | 验证集损失：343.8906\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：367.2535 | 验证集损失：344.2161\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.36s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：373.4237 | 验证集损失：343.5097\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.40s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集损失改进：342.1362\n",
      "保存最优模型，当前验证集损失：342.1362\n",
      "训练集损失：379.0205 | 验证集损失：342.1362\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9/9 [00:12<00:00,  1.38s/it]\n",
      "Validation: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集损失：379.1552 | 验证集损失：343.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- 5. 训练与验证流程 --------------------------\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets, _ in tqdm(loader, desc=\"Training\"):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播+优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():  # 验证时禁用梯度计算\n",
    "        for images, targets, _ in tqdm(loader, desc=\"Validation\"):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "# 开始训练\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate_one_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # 学习率调度\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # 保存最优模型（基于验证集损失）\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        print(f\"验证集损失改进：{best_val_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), \"best_resnet50_biomass.pth\")\n",
    "        print(f\"保存最优模型，当前验证集损失：{best_val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"训练集损失：{train_loss:.4f} | 验证集损失：{val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载最优模型完成\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huimi\\AppData\\Local\\Temp\\ipykernel_26892\\2803385875.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_resnet50_biomass.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- 6. 测试集预测与提交文件生成 --------------------------\n",
    "def predict_test(model, loader, device, target_names):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "    with torch.no_grad():\n",
    "        for images, sample_id in tqdm(loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            print(\"outputs:\",outputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            sample_ids.extend(sample_id)\n",
    "    \n",
    "        print(\"predictions:\",predictions)\n",
    "        print(\"sample_ids:\",sample_ids)\n",
    "\n",
    "    # 整理提交格式：sample_id__target_name, target\n",
    "    submission_list = []\n",
    "    for idx, sid in enumerate(sample_ids):\n",
    "        pred = predictions[idx]\n",
    "        for i, tn in enumerate(target_names):\n",
    "            submission_list.append({\n",
    "                'sample_id': sid,\n",
    "                'target': pred[i]\n",
    "            })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_list)\n",
    "    # 按sample_id排序\n",
    "    submission_df = submission_df.sort_values(\"sample_id\").reset_index(drop=True)\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "\n",
    "# # 加载最优模型进行预测\n",
    "model.load_state_dict(torch.load(\"best_resnet50_biomass.pth\"))\n",
    "print(\"加载最优模型完成\")\n",
    "# 生成提交文件\n",
    "print(len(test_loader.dataset))\n",
    "# submission_df = predict_test(model, test_loader, device, target_names)\n",
    "# submission_df.to_csv(\"./submission.csv\", index=False)\n",
    "# print(\"提交文件生成完成！\")\n",
    "# print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后行数：1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 步骤1：提取纯样本ID\n",
    "test_df[\"actual_sample_id\"] = test_df[\"sample_id\"].str.split(\"__\").str[0]\n",
    "\n",
    "# 步骤2：按纯样本ID去重，保留每组第一行（keep=\"last\" 保留最后一行，效果一致）\n",
    "test_df_merged = test_df.drop_duplicates(subset=\"actual_sample_id\", keep=\"first\")\n",
    "\n",
    "# 验证\n",
    "print(f\"合并后行数：{len(test_df_merged)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
